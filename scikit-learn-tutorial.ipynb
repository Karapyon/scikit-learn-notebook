{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "機械学習とは、その名の通り「機械」を「学習」させることで、あるデータに対して予測を行えるようにすることです。\n",
    "\n",
    "* 機械とは、具体的には数理・統計的なモデルになります。\n",
    "* 学習とは、そのモデルのパラメータを、実際のデータに沿うよう調整することです。\n",
    "\n",
    "![](./pictures/machine_learning.PNG)\n",
    "\n",
    "学習の方法は大きく分けて2つあります。  \n",
    "\n",
    "* 教師有り学習(Supervised learning): データと、そこから予測されるべき値(正解)を与えることで学習させます。\n",
    " * 分類(Classification): データがいくつかのカテゴリに分類できるとき、そのカテゴリを予測させます(例：手書きの数字が0～9の何れかであるか判別するなど)\n",
    " * 回帰(Regression): データから予測される連続的な値を予測します(例：年齢と体重から慎重を予測するなど)。\n",
    "* 教師なし学習(Unsupervised learning): データを与えることで、その裏側にある構造を学習させます\n",
    " * クラスタリング: 似ているデータをまとめることで、データがどれくらいの集合(クラスタ)から構成されるのかを予測します。\n",
    " * 分布推定： データを生み出している確率分布の推定を行います。\n",
    "\n",
    "[scikit-learn](http://scikit-learn.org/stable/index.html)は、Python製の機械学習ライブラリです。  \n",
    "この中には様々な「機械」が実装されており、その「学習」のための仕組みも備わっています。  \n",
    "\n",
    "以下では、このscikit-learnを利用しデータを準備するところから実際にモデルを構築し学習・評価を行うまでの手順を解説します。\n",
    "\n",
    "1. [データの準備](#Loading-the-Data) \n",
    "2. [データの整備](#Arrange-the-Data) \n",
    "3. [モデルの選択](#Select-the-Model)\n",
    "4. [データの分割](#Split-the-Data) \n",
    "5. [モデルの学習](#Training-the-Model)\n",
    "6. [モデルの保管](#Store-the-Model)\n",
    "\n",
    "環境のセットアップについては、以下にまとめてあるのでご参考ください。\n",
    "\n",
    "[Pythonで機械学習アプリケーションの開発環境を構築する](http://qiita.com/icoxfog417/items/950b8af9100b64c0d8f9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# enable showing matplotlib image inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learnでは、よく例として利用されるデータセット(irisのデータや手書き文字のデータなど)を以下のように簡単に取得することができます。\n",
    "\n",
    "[Dataset loading utilities](http://scikit-learn.org/stable/datasets/index.html#datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dataset`は、以下の内容で構成されています。\n",
    "\n",
    "* `data`: データ本体(常にサンプル×特徴量の二次元配列。画像などデータ自体が2次元で表示される場合は、`images`からアクセスできる)\n",
    "* `target`: データから予測されるべき正解(教師データ)\n",
    "* `feature_names`: 特徴量項目の名前\n",
    "* `target_names` : 予測値項目の名前\n",
    "* `DESCR`: データの説明"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['target', 'feature_names', 'target_names', 'data', 'DESCR'])\n"
     ]
    }
   ],
   "source": [
    "print(iris.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通常のデータ読み込みには、Pythonに標準で搭載されている`csv`などが使えます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1' 'good']\n",
      " ['2' 'bad']\n",
      " ['3' 'vgood']]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "encoding = \"utf-8\"\n",
    "ratings = []\n",
    "with open(\"./data/ratings.txt\", encoding=\"utf-8\") as f:\n",
    "    content = csv.reader(f, delimiter=\"\\t\")\n",
    "    lines = list(content)\n",
    "    ratings = np.array(lines)\n",
    "\n",
    "print(ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "なお、このようなデータの読み込み、また読み込んだデータに対する操作をサポートするライブラリとして[pandas](http://pandas.pydata.org/)があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arrange the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データの中の各特徴量は、別々の平均・分散を持っています(例: 体重と身長では平均も分散も異なる)。  \n",
    "この状態だと学習が効率的に進まないため、各特徴量の平均を0・分散を1にそろえる正規化(Normalization)を行うことが一般的です。  \n",
    "(これに加え、特徴量間の相関を消す白色化(Whitening)まで行うこともあります)。  \n",
    "\n",
    "scikit-learnでは[`preprocessing`](http://scikit-learn.org/stable/modules/preprocessing.html)を使用することでこの作業をとても簡単に行うことができます。以下では、`StandardScaler`を使って処理を行っています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before scaling:\n",
      " {'mean': array([ 5.84333333,  3.054     ,  3.75866667,  1.19866667]), 'std': array([ 0.82530129,  0.43214658,  1.75852918,  0.76061262])}\n",
      "After scaling (mean is almost 0, std = 1):\n",
      " {'mean': array([ -1.69031455e-15,  -1.63702385e-15,  -1.48251781e-15,\n",
      "        -1.62314606e-15]), 'std': array([ 1.,  1.,  1.,  1.])}\n",
      "Inverse the scaling:\n",
      " {'mean': array([ 5.84333333,  3.054     ,  3.75866667,  1.19866667]), 'std': array([ 0.82530129,  0.43214658,  1.75852918,  0.76061262])}\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "iris_data = iris[\"data\"]\n",
    "scaler = preprocessing.StandardScaler().fit(iris_data)\n",
    "describe = lambda t, x: (t + \":\\n {0}\").format({\"mean\": np.mean(x, axis=0), \"std\": np.std(x, axis=0)})\n",
    "\n",
    "# before scaling\n",
    "print(describe(\"Before scaling\", iris_data))\n",
    "\n",
    "# scaling\n",
    "iris_data_scaled = scaler.transform(iris_data)\n",
    "print(describe(\"After scaling (mean is almost 0, std = 1)\", iris_data_scaled))\n",
    "\n",
    "# inverse\n",
    "iris_data_inv = scaler.inverse_transform(iris_data_scaled)\n",
    "print(describe(\"Inverse the scaling\", iris_data_inv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※`preprocessing`には`Normalization`というモジュールがありますが、これは一般的に言う正規化を行うためのものではないので注意してください。  \n",
    "\n",
    "また、データの中にはテキストである項目が含まれていることもあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1' 'good']\n",
      " ['2' 'bad']\n",
      " ['3' 'vgood']]\n"
     ]
    }
   ],
   "source": [
    "print(ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記の`good`などのテキスト項目は、最終的には数値にしないとデータを学習させることができません。  これも`preprocessing`を利用することで簡単に数値へ変換することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good' 'bad' 'vgood'] is encoded to [1 0 3]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit([\"bad\", \"nbad\", \"good\", \"vgood\"])\n",
    "\n",
    "encoded_rating = le.transform(ratings[:, 1])\n",
    "print(\"{0} is encoded to {1}\".format(ratings[:, 1], encoded_rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Feature Extraction](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction)では、テキスト/画像についてより強力に特徴量の数値化(ベクトル化)を行う機能がサポートされています。以下では、cityというテキストの項目がDubai/London/San Fransiscoを表す0/1の特徴量へと変換されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.   0.   0.  33.]\n",
      " [  0.   1.   0.  12.]\n",
      " [  0.   0.   1.  18.]\n",
      " [  1.   0.   0.  32.]]\n",
      "['city=Dubai', 'city=London', 'city=San Fransisco', 'temperature']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "measurements = [\n",
    "    {\"city\": \"Dubai\", \"temperature\": 33.},\n",
    "    {\"city\": \"London\", \"temperature\": 12.},\n",
    "    {\"city\": \"San Fransisco\", \"temperature\": 18.},\n",
    "    {\"city\": \"Dubai\", \"temperature\": 32.},\n",
    "]\n",
    "\n",
    "vec = DictVectorizer()\n",
    "vectorized = vec.fit_transform(measurements).toarray()\n",
    "print(vectorized)\n",
    "\n",
    "feature_names = vec.get_feature_names()\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`preprocessing`には、他にも欠損値の修正を行う`Imputer`などデータの整備に役立つモジュールが含まれています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データを図示することは、この後のモデルの選択を行う時を含め、様々なシーンで非常に重要です。\n",
    "しかし、単純に特徴量が4つになっただけでもデータを図示することができなくなってしまいますし(4次元の図になってしまうため)、場合によっては非常に多くなることもあります(テキスト解析など)。\n",
    "\n",
    "そのため、データをなるべく少ない、必要最小限の特徴量で表現することが重要になります。これを行うのがDimensionality reduction(次元削除/次元圧縮)と呼ばれる手法です。\n",
    "\n",
    "具体的には、データの中に身長と体重があった場合、これらは体が大きくなれば両方とも増える特徴量のため、データの特性を表す上ではどちらかひとつで十分です。このように片方が増えれば片方も増えるといった、互いに相関のある特徴量を消していけば必要最小限の特徴量でデータを表現することができる・・・というのが基本的な考え方です。\n",
    "\n",
    "scikit-learnでは[`decomposition`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition)を利用しこの処理を行うことができます。以下では、[TruncatedSVD](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD)によって数字データの特徴量を、上記で述べたとおり互いに相関のない、2つの特徴量へと圧縮しています。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension is reduced from 64 to 2.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import decomposition\n",
    "\n",
    "digits_data = digits[\"data\"]\n",
    "show_dimension = lambda dset: len(dset[0])\n",
    "\n",
    "dimension = 2\n",
    "digits_recuced = decomposition.TruncatedSVD(n_components=dimension).fit_transform(digits_data)\n",
    "\n",
    "print(\"Dimension is reduced from {0} to {1}.\".format(show_dimension(digits_data), show_dimension(digits_recuced)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "機械学習に使えるモデルには色々なものがあり、scikit-learnでも様々なモデルが使えるようになっています。  \n",
    "ただ、その分一体どれを選べば良いのかは非常に悩ましい問題です。  \n",
    "\n",
    "一つの基準として、以下のようなフローチャートがあります。これは、scikit-learnの中のアルゴリズムをどのような基準で選択したらよいのかを図示したものです。\n",
    "\n",
    "[Choosing the right estimator](http://scikit-learn.org/stable/tutorial/machine_learning_map/)\n",
    "\n",
    "scikit-learnにはNeural Networkがないため図中にもありませんが、基本的にはSVC/SVRの代替であり、データが多いほど精度が向上します。\n",
    "\n",
    "ポイントとしては以下になります。\n",
    "\n",
    "* 最低でも50件以上はデータを集める\n",
    "* 単純なモデルから始める(ClassificationならLinerSVC、RegressionならRasso/ElasticNetなど)\n",
    "* Just lookingから始める(データを見て、必要に応じ次元削除を行う)\n",
    "\n",
    "機械学習で正しい結果を出すにはデータの整備(図中ではJust looking、[前章](#Arrange-the-Data)に当たる部分)が欠かせません。データ整備した上で単純なモデルで検証をしてみて、必要に応じ他のモデルを試していくというのが基本的な進め方になります。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Model Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特徴量が多い場合は、どの特徴量をモデルに使うのかも重要な問題です。scikit-learnには、どの特徴量が予測値に寄与しているか調べるための機能があります。  以下では、[Feature selection](http://scikit-learn.org/stable/modules/feature_selection.html)を利用し特徴量をもっとも有用な2つに絞っています(k=2)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "X, y = iris.data, iris.target\n",
    "print(X.shape)\n",
    "\n",
    "X_new = SelectKBest(chi2, k=2).fit_transform(X, y)\n",
    "print(X_new.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習に当たっては、データを学習用(training set)と評価用(test set)に分けておきます。学習に使ったデータに対して予測がうまくできるのは当たり前なので、正確に精度を測定するため、評価用のデータは学習用とは別にしておきます。\n",
    "\n",
    "単純に学習用と評価用に2分割するのでなく、データ全体を何個かに分割し、評価用として使うデータを切り替えていくという方法もあります。これにより、少ないデータでも効率的に学習を行うことができます。\n",
    "\n",
    "[K-FOLD CROSS-VALIDATION, WITH MATLAB CODE](https://chrisjmccormick.wordpress.com/2013/07/31/k-fold-cross-validation-with-matlab-code/)\n",
    "\n",
    "![cross_validation](./pictures/cross_validation.PNG)\n",
    "\n",
    "これはCross Validationと呼ばれる手法ですが、scikit-learnでは単純な分割からこのCross Validationまで、[`cross-validation`](http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation)で行えるようになっています。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data is 30.0% of data\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "\n",
    "test_size = 0.3  # use 30% of data to test the model\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(iris.data, iris.target, test_size=test_size, random_state=0)\n",
    "test_data_rate = X_test.shape[0] * 100 / (X_train.shape[0] + X_test.shape[0])\n",
    "\n",
    "print(\"test data is {0}% of data\".format(test_data_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validationを利用する際は、データの分割と学習を合わせて行ってくれる`cross_val_score`を利用するのが簡単ですが(後述します)、データの分割のみ行う場合は`KFold`を利用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: training 100, test 50\n",
      "1: training 100, test 50\n",
      "2: training 100, test 50\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "length = len(iris.target)\n",
    "split_count = 3  # divide into 3 set\n",
    "kf = KFold(length, n_folds=split_count)\n",
    "\n",
    "for i, train_test_indexes in enumerate(kf):\n",
    "    train, test = train_test_indexes\n",
    "    x_train = iris.data[train]\n",
    "    y_train = iris.target[train]\n",
    "    \n",
    "    x_test = iris.data[test]\n",
    "    y_test = iris.target[test]\n",
    "\n",
    "    print(\"{0}: training {1}, test {2}\".format(i, len(y_train), len(y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これでデータの準備は整ったので、いよいよ学習を行っていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下では、分類を行う際によく利用される[Support Vector Machine](http://scikit-learn.org/stable/modules/svm.html)をベースにその学習方法などを解説していきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(gamma=0.001, C=100.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルの構築はたったこれだけでおしまいです。そして、学習もたった一行で済ませることができます(以下の例では、最後の1データ以外を学習データとして渡しています)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,\n",
       "  gamma=0.001, kernel='rbf', max_iter=-1, probability=False,\n",
       "  random_state=None, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(digits.data[:-1], digits.target[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "そして、取っておいた最後の一つのデータについて、モデルを使って予測させてみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(digits.data[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際判定した画像は以下です。「8」という予測はそこそこ的を得ているのではないかと思います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAL8AAADDCAYAAADTCsC8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxXL4rVgOvxXL4bdiOfxWrP8CEMz+8G2LElYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xb1594d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(1, figsize=(3, 3))\n",
    "plt.imshow(digits.images[-1], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今度はCross Validationを使ってみます。`cv`ではデータの分割数(foldの数)を指定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.97 (+/- 0.03)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "\n",
    "scores = cross_validation.cross_val_score(clf, digits.data, digits.target, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記ではモデルのパラメータを固定で指定しましたが(`gamma=0.001`など)、実際どんなパラメータを設定すべきかは非常に悩ましい問題です。  \n",
    "最適なパラメータを探すため、各パラメータが取りうる範囲を決め、その組み合わせを試していくという手法があります。これをGrid Searchと呼びますが、scikit-learnではこれを行うための[Grid Search](http://scikit-learn.org/stable/modules/grid_search.html#grid-search)モジュールが提供されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=10, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.001,\n",
      "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False)\n",
      "0.972 (+/-0.008) for {'kernel': 'rbf', 'gamma': 0.001, 'C': 1}\n",
      "0.947 (+/-0.011) for {'kernel': 'rbf', 'gamma': 0.0001, 'C': 1}\n",
      "0.973 (+/-0.007) for {'kernel': 'rbf', 'gamma': 0.001, 'C': 10}\n",
      "0.959 (+/-0.012) for {'kernel': 'rbf', 'gamma': 0.0001, 'C': 10}\n",
      "0.973 (+/-0.007) for {'kernel': 'rbf', 'gamma': 0.001, 'C': 100}\n",
      "0.963 (+/-0.009) for {'kernel': 'rbf', 'gamma': 0.0001, 'C': 100}\n",
      "0.949 (+/-0.010) for {'kernel': 'linear', 'C': 1}\n",
      "0.949 (+/-0.010) for {'kernel': 'linear', 'C': 10}\n",
      "0.949 (+/-0.010) for {'kernel': 'linear', 'C': 100}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "candidates = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4], 'C': [1, 10, 100]},\n",
    "              {'kernel': ['linear'], 'C': [1, 10, 100]}]\n",
    "\n",
    "clf = GridSearchCV(SVC(C=1), candidates, cv=5, scoring=\"f1\")\n",
    "clf.fit(digits.data, digits.target)\n",
    "\n",
    "print(clf.best_estimator_)\n",
    "\n",
    "for params, mean_score, scores in clf.grid_scores_:\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean_score, scores.std() / 2, params))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Lerning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習の結果、優秀なモデルができることもあればそうでないこともあります。\n",
    "特徴量が増える、つまり多次元になるほど「よさそうに見えるパラメーターの組み合わせ」は増えていくので、何度も学習させてみないとなかなか最適と思える結果にはたどり着けなくなります。\n",
    "\n",
    "こうした問題に対応するため、複数のモデルを別々に学習させ、最終的にはそれらの組み合わせで決めることで精度を上げるという手法があります。  \n",
    "これがアンサンブル学習と呼ばれる手法です。「複数のモデル」は、同じモデルのこともあれば(Bagging)それぞれ異なるモデルを使うこともあります(Boosting)。\n",
    "\n",
    "scikit-learnでは、[ensemble](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble)によってこのアンサンブル学習を行うことができます。以下では、Baggingによって10個のモデルを並列で学習させ、その組み合わせで決めるモデルを作成しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98 (+/- 0.03)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "base_clf = svm.SVC()\n",
    "bagging_clf = BaggingClassifier(base_estimator=clf, n_estimators=10, max_samples=0.9, max_features=2, n_jobs=4)\n",
    "\n",
    "scores = cross_validation.cross_val_score(clf, iris.data, iris.target, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`n_jobs`により並列で学習させる際のプロセス数を簡単に調整することができ、高速な学習が可能です"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習したモデルはファイルとして出力し保存しておくことができます。  \n",
    "以下は、標準の`pickle`を使う手法です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "\n",
    "clf = svm.SVC()\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "clf.fit(X, y)\n",
    "\n",
    "import pickle\n",
    "s = pickle.dumps(clf)  #serialize model data\n",
    "clf2 = pickle.loads(s) #load serialized model data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このほか、`sklearn.externals`の`joblib`を利用しファイルに保管することもできます。大規模なモデルなどにはこちらの方がよいでしょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "joblib.dump(clf, \"data/model.pkl\") \n",
    "clf = joblib.load(\"data/model.pkl\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上で解説は終了です。scikit-learnは[公式ドキュメント](http://scikit-learn.org/stable/index.html)が充実しているので、そちらもご参照ください。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
